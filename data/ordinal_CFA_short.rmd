---
title: "ordinal_CFA"
output: html_notebook
---

This is the evaluation of the experiment's results. The focus of this notebook is on ordinal confirmatory factor analysis (CFA) in preparation of (M)ANOVA.

This notebook expands the master thesis by data analysis and respective code as well as short explanations. This markdown does not replace the thesis, it serves solely as an appendix.

<h1>Library Downloads</h1>
```{r}
library(tidyverse)
library(broom)
library(ggplot2)
library(dplyr)
library(finalfit)
library(mice)
library(miceadds)
library(MASS)
library(lavaan)
library(lavaan.mi)
library(semptools)
library(semTools)
library(psych)
library(corrplot)
```

<h1>Sample Load</h1>
First, the previously imputed data is loaded as a mids object:

```{r}
getwd()
final_imputed_data <- read_rds("data/mi_mids_object.rds")
glimpse(final_imputed_data)
```

Subsequently, the imputed data is reduced onto the for the CFA analysis relevant columns, i.e., items. The reduction is completed by using the lapply() function that allows to turn the mids object into the subset/list of items. Thus, the analysis still represents the missingness in response through directly using the multiple imputed data which represents the uncertainty as outlined, previously (Li, Stuart and Allison, 2015).

```{r}
relevant_columns <- c("AC01_01", "AC01_02", "AC01_03", "DI01_01", "DI01_02", "DI01_03", "DI01_04", "UI01_01", "UI01_02", "UI01_03", "UI01_04", "EX01_01", "EX01_02", "EX01_03", "EX01_04")

list_of_rel_cols <- lapply(complete(final_imputed_data, "all"), function(df) df[, relevant_columns])

glimpse(list_of_rel_cols)
```

<h1>Ordinal Confirmatory Factor Analysis</h1>

In order to conduct the ordinal CFA, the following sequence of steps is applied and analyzed:

<ol>
<li>Polychoric Correlation Matrix</li>
<li>Four Factor ordinal CFA with WLSMV</li>
<li>Analysis of fit indices and parameters</li>
<li>Adjustment of the four factor model</li>
<li>Model comparison: four factor vs. three factor model</li>
</ol>

<h2>Polychoric Correlation Matrix</h2>

In the following the polychoric correlation matrix is constructed and interpreted. The Pearson correlation coefficients are not used since we are analyzing the item response format of an ordinal scale, i.e., on a micro-level, where parametric techniques do not apply (Cleff, 2019; Carifio and Perla, 2007). This is underpinned by the violated assumption of Pearson that requires metric scales and linear relationships which are not prevalent due to the response-level analysis (Cleff, 2019). 

First, the polychoric correlation is applied to the list of imputed item data. To analyze the relationship between ordinal variables, Spearman's Rho is calculated (Cleff, 2019).

```{r}
cor_list_rel_cols <- lapply(list_of_rel_cols, function(df) polychoric(df)$rho)
cor_list_rel_cols
```

Since the correlation has been applied to the imputed data, one correlation point estimate must be pooled according to Van Buuren and Groothuis-Oudshoorn (2011). It has to noted that the following custom way of pooling via mean averaging does not fully pursue Rubin's rules. This is due that only the point estimate and not further statistical measures such as the standard error are considered (Rubin, 1987; Van Buuren and Groothuis-Oudshoorn, 2011). Another reason for this limited approach is that neither the mice nor the lavaan package deliver such functionality for polychoric correlation with imputed data at the date of this analysis (October/November 2025) (Van Buuren and Groothuis-Oudshoorn, 2011; Jorgensen, 2025). Acknowledging this limitation, the correlation analysis is continued on the basis of the point estimates.

```{r fig.width=8, fig.height=8}
p_cor_pooled <- Reduce("+", cor_list_rel_cols) / length(cor_list_rel_cols)
dimnames(p_cor_pooled) <- list(
  Rows=relevant_columns, 
  Cols=relevant_columns
  )

corrplot(p_cor_pooled, method = 'circle', type = 'lower', insig='blank', addCoef.col ='black', number.cex = 0.9, tl.col = 'black', tl.cex = 0.9)
```

At a first glance, the correlations between DI01_03 and DI01_02, EX01_04 and DI01_02 as well as EX01_04 and DI01_03 are close to + 1 indicating a monotonic, positive relationship. This indicates that if one of the items in the relationship has a high rating (close to 5), the other one has a high rating, too. This points to a lack of discriminant validity due to a potential multicollinearity as those factor correlations exceed 0.85 (Brown, 2015). As the latter ordinal CFA necessitates discriminant factors that are parsimonous (Brown, 2015), these correlations must be acknowledged later on.

As four factors: accountability, disclosure, usefulness of information and explainability emerged upon literature, the items of each factor should have at least a moderate correlation > 0.5. This can partially be inspected by a stair-like blue pattern of almost all correlations between the items of the same dimension, e.g., AC01_01 - AC01_03. Especially for the accountability and disclosure factor the blue steps of the stair can be observed. In contrast, in most parts the correlation between items of different factors is rather low except for the items of usefulness and explainability which have both a moderate correlation around 0.5 with accountability items and disclosure items.

Not all items of one dimension have a high correlation with one another. In light of this, it has to be considered that the treatments impact the correlation, too. As the different treatments aim to steer different behaviour against the backdrop of transparency and thus, do not always aim to achieve a high score across all items of a factor. For instance, UI01_04 has a rather low even negative correlation with UI01_01 and UI01_02. While the Chain-of-Thought-based treatments aim to actively interact with the user, Vanilla- and RAG-based treatments do not actively aim for an interaction but still explain the most important points.

<h2>Ordinal CFA with WLSMV</h2>

As we have inspected the polychoric correlation between the items of each factor, we aim to check the item assignment to factors using an ordinal CFA (Brown, 2015). Thus, we aim to check the hypothesis of:

H0: The input variance-covariance matrix is reproduced by the model implied variance-covariance matrix. (Brown, 2015)

In contrast to other significance tests, the goal is not to reject the null hypothesis, but to support it by showing its non-significance (Kline, 2016). The significance level is set to 0.05, i.e., we aim to achieve a p-value > than 0.05.

Since we are conducting ordinal variables in their response format (Carifio and Perla, 2007), with a rather small sample we use the robust "Weighted Least Square Means and Variances" estimator (WLSMV) (Brown, 2015; Bean, 2021). In the following the factor loadings for the four literature-founded factors of accountability, disclosure, usefulness, and explainability are examined.

First, the subset list of the imputed items of these factors has to be transformed back into an mids object. In contrast to the prior correlation evaluation, using the mids object ensures that the standard errors as well as further statistical measures influenced by multiple imputation are considered in terms of a mira object (Van Buuren and Groothuis-Oudshoorn, 2011). Thus, m = 10 ordinal CFA analysis are conducted and pooled after analysis (Van Buuren and Groothuis-Oudshoorn, 2011; Jorgensen, 2025).

```{r}
mids_rel_cols <- datlist2mids(list_of_rel_cols)
glimpse(mids_rel_cols)
```

With this mids object, the ordinal CFA can be conducted in the following with the theory-founded model of four factors with at least three items: 

```{r}
transparency <- 
" accountability =~ AC01_01 + AC01_02 + AC01_03 
disclosure =~ DI01_01 + DI01_02 + DI01_03 + DI01_04 
usefulness =~ UI01_01 + UI01_02 + UI01_03 + UI01_04 
explainability =~ EX01_01 + EX01_02 + EX01_03 + EX01_04 "

ordinal_CFA1 <- cfa.mi(transparency, data=mids_rel_cols, estimator = "WLSMV", mimic = "MPlus", std.lv = TRUE, ordered = TRUE)
```

<h2>Analysis of parameters and goodness-of-fit indices followed by model adjustments</h2>

The warnings already indicate that the covariance matrix is a non-positive definite. However, "the input variance-covariance matrix and the model-implied variance-covariance matrix are" (Brown, 2015, p. 162) required to be positive definite for analyzing the factor loadings correctly (Brown, 2015; Kline, 2016). Consequently, we are analyzing the model in order to respecify it in line with the at hand statistics and the literature findings of the thesis.

```{r}
summary(ordinal_CFA1, standardized = TRUE, fit.measures = TRUE)
```

We analyze the absolute fit indices (chi-square, degrees of freedom (df), p-value, standardized root mean square (SRMS)), the parsimony correction index (root mean square error of approximation (RMSEA)), and the comparative fit indices (comparative fit index (CFI), Tucker-Lewis index (TLI)) to evaluate the null hypothesis. In addition, we inspect the residual correlations and modification indices where we also conduct a Wald test in order to inspect which items can be dropped. We conclude by interpreting the parameter estimates to finally shed a light on how the respecifyied model looks like. (Brown, 2015)

First, we analyze the absolute fit indices. For the chi-square value, the scaled chi-square ("Test statistics", column "Scaled") is considered (Kline, 2016). The model specified achieves are rather bad scaled chi-square with ≈ 217.856 which is higher than the baseline model of ≈ 216.225 that rejects the null hypothesis (Brown, 2015). Moreover, the user model has 84 df whereas the baseline has 105 df, i.e., a broader freedom of estimates - even though the user model has less, it achieves a higher chi-square value (Brown, 2015). Both models have p-values < 0.05, i.e., reject the null hypothesis. As a consequence the model implied variance-covariance matrix does not fit the input matrix.

Subsequently, the scaled SRMR of ≈ 0.175 indicates a rather high deviation between the correlations of the input and implied matrix whereas values close to 0.08 indicate an acceptable discrepancy (Brown, 2015). A perfect fit is achieved if SRMR = 0 (Brown, 2015). To further check on this, the residual correlations are inspected later on.

Commencing with the parsimony correction index for WLSMV, the robust RMSEA, that states ≈ 0.192 and thus, a model misspecification even in the lower confidence interval with ≈ 0.160. The model does not fit the chi-square value, i.e., the population. For a good fit the RMSEA value is suggested to be < 0.05. A perfect fit is achieved if RMSEA = 0. (Brown, 2015)

For the comparative fit indices for WLSMV, the robust CFI of ≈ 0.708 already indicates what have become apparent through the prior indices - a poor model fit. The TLI even reinforces this with a value of ≈ 0.635. A perfect fit is achieved if CFI and TLI are close to 1. (Brown, 2015)

As these global indices have been evaluated with the result of a poor model fit that must be respecified, the model is evaluated against local areas of strain to check if the relationships of the input data have been reproduced properly. Therefore, the standardized residual variance-covariance matrix is examined first, followed by an analysis of the modification indices. (Brown, 2015)

```{r}
# cor.bentler residuals, i.e., standardized ones - but $cov.z is needed to show the standardized matrix
res_va_cov <- lavResiduals.mi(ordinal_CFA1, zstat = TRUE)
z_res_va_cov <- lavResiduals.mi(ordinal_CFA1, zstat = TRUE)$cov.z

# raw covariance residuals 
#lavResiduals.mi(ordinal_CFA1, type = "raw", zstat = FALSE)$cov
```

A visualization of the non-standardized residual variance-covariance matrix values already provides hints for critical values that are "greater than the absolute value of 1.96 [...] [which] corresponds to a statistical significant z score at p < .05 ([...] often rounded up to 2.00)." (Brown, 2015, p. 99) (Brown, 2015). 

```{r fig.width=8, fig.height=8}
corrplot(res_va_cov$cov, method = 'circle', type = 'lower', insig='blank', addCoef.col ='black', number.cex = 0.9, tl.col = 'black', tl.cex = 0.9)
```
In the following the z score, i.e., the standardized residual variance-covariance values are provided. Positive standardized residuals indicate that the model underestimates the variances-covariances of the input matrix (Brown, 2015). Whereas negative standardized residuals point on an overestimation (Brown, 2015). Against the backdrop of this considerations as well as the threshold of |2.00| the values are evaluated for each factor.

```{r}
z_res_va_cov
```

The matrix shows a broad variation of over- (if negative) and underestimation (if positive) of relationships ranging from ≈ -5.060 for EX01_04 and EX01_03 (overestimation) to ≈ +5.579 for UI01_04 and UI01_02 (underestimation). It has to be noted that the relationships between the items of AC01 factor which stands for accountability and the other items of the same and other factors exceeds the threshold of |2.00| only 9 times out of 39 relationships excluding the 0.0 values to the item itself. In contrast, the relationships of the items of the other factors, especially of UI01 and EX01, are more often and more severely over- and underestimated, i.e., exceeding the threshold of |2.00|. As a consequence the model estimated relationships amongst items suggest a statistically significant discrepancy of the model and thus, a poor fit. This results are a first sign to reflect on the items of the factors DI01, UI01, and EX01.

Especially parameters with relatively "large, negative standardized residuals" (Brown, 2015, p. 106) often indicate that they are unnecessary and can be potentially dropped (Brown, 2015). Against this background, the necessity of EX01_04 as the most negative standardized residual (≈ -5.060) must be reflected on.

Also the modification indices (MIs) are inspected in order to explore the potential decrease of the model's chi-square if fixed parameters are freely estimated or fixed to a certain value (Brown, 2015). The modification tabular below provides an outlook on potential scenarios if an item would load onto another factor (cross-loading) or how the residual variance-covariance values would change. However, as cross-loadings are permitted in CFA (Kline, 2016), these cases are evaluated as if the respective item would be reassigned. According to Brown (2015), only MIs with an value > 3.84 allow for a significant improvement for p < 0.05 - this value is rounded to 4 for the at hand evaluations. 

Moreover, the epc columns epc, sepc.lv, and sepc.all depict the expected parameter change (Brown, 2015). As sepc.all depicts the standardized epc it is considered only. In line with Brown (2015), only positive sepc.all values are considered against the backdrop of potential Heywood cases of statistically impossible negative resulting factor loadings (Brown, 2015).

Due to multiple imputation, the relative increase in variance (riv) as well as the FMI are shown, too. However, as the multiple imputation has been analyzed with an approximate FMI based on continuous values, the at hand FMI must be cautioned but can be disregarded for the at hand observations (Von Hippel, 2018). This is due to this FMI is founded on ordinal data for which the quadratic rule does not apply (Von Hippel, 2018).

Solely, the combination of MIs and sepc.all values is essential for evaluation from this point of view, for the case that the suggested modification is considered in combination with all prior observations of this initial CFA.

```{r}
mod_ind <- modificationIndices.mi(ordinal_CFA1, pool.method = c("D2"), standardized = TRUE, cov.std = TRUE, sort. = TRUE, minimum.value = "4")
mod_ind
view(mod_ind)
```

First, the potentially to-be reassigned items are introduced. The first three MIs ranging from ≈ 99.071 to ≈ 69.380 suggest a loading of UI01_04 on all three other dimensions. Although only the suggested modification of assigning UI01_04 to disclosure would be statistically meaningful due to the highest MI ≈ 99.071 despite the lowest sepc.all ≈ 0.979 as the sepc.all values are close to one another (≈ 1.001, ≈ 1.977) compared to the range of MIs (≈ 93.401, ≈ 69.380). The loading of UI01_01 to disclosure is also considered as it has an significant MI of ≈ 41.524 with an sepc.all of ≈ 0.579. Moreover, the loading of EX01_04 to disclosure has a MI of ≈ 7.294 and a rather bigger positive sepc.all of ≈ 1.516. Whereas the loading of EX01_04 on usefulness is not considered as the previous suggestion is stronger in terms of MI (≈ 7.294 > ≈ 5.644) and sepc.all (≈ 1.516 > ≈ 0.288).

Second, the impact of changes in covariances are explained. Only those with a MI >= ≈ 8.0 and/or a sepc.all that is > ≈ |2.0| are explained in detail.

DI01_03 and UI01_04 as well as DI01_04 and UI01_03 have an MI of ≈ 9.957 and ≈ 9.857 indicating a high covariance with sepc.all ≈ 1.864 and ≈ 0.583 stemming from the fact of a similar wording and context. For instance for DI01_03 and UI01_04 which aim at the one hand for providing direct access additional sources (DI01_03) and requesting the user to interact with the information (UI01_04). The similarity of the factor items of DI01 and UI01 is underpinned by the high MI of DI01_04 and UI01_01 and DI01_01 and UI01_03 with ≈ 8.304 and ≈ 8.299 each and again a moderate sepc.all with ≈ 0.594 and ≈ 0.385. This is again due to the similar wording and aim of the factors which relies in advancing the user's understanding of a topic through a broad access to sources which shall be processed by the LLM in a user-understandable way. 

Further, the covariance of DI01_02 and UI01_01 with an MI of ≈ 6.619 and a sepc.all of ≈ 2.808 has to be noted indicating an relative high expected parameter change due to a similar interest as users conclude that an LLM deliver only all necessary information for a topic if the references are provided, too. A similar relationship is depicted by the covariance of DI01_02 and EX01_04 with a MI of ≈ 5.625 and a sepc.all of ≈ 3.415 which both refer to the access of resources used. Another example is the covariance of DI01_02 and UI01_04 with an MI of ≈ 5.152 and a sepc.all of ≈ 3.035 through which is indicated that the accessibility of resources used also invites the user to actively intervene.

There is also a high covariance and thus, similarity of items between UI01_03 and UI01_04 with an MI of ≈ 8.0 and a sepc.all of ≈ 0.575 indicating a similar effect in aiming for an increased user understanding through interventions and adjusting to the user.

Despite the previous observations there are also strong negative sepc.all values. For instance for the covariance of DI01_02 and EX01_03 (sepc.all ≈ -2.75) as well as DI01_02 and EX01_02 (sepc.all ≈ -2.049) with an MI of ≈ 6.581 and ≈ 6.19. This is representative for the assumption that there is almost no meaningful relationship between the item of DI01_02 and the items EX01_02/03 as respondents do not associate revealing sources as describing how data was processed by the LLM to give an answer.

However, there are further positive covariances not only tangling the factors UI and DI. For instance the covariance of AC01_03 and UI01_03 as both items tangle the model's adjustment to at the one hand side the user's guidelines and at the other hand side to the user. This is reflected by a MI of ≈ 8.848 and a positive but smaller sepc.all (≈ 0.380) compared to the previous examples. 

Lastly, the non-considered modifications are outlined. The assignment of EX01_03 to usefulness is disregarded due to an negative sepc.all indicating "no meaningful direct relationship" (Brown, 2015, p. 102) despite an MI of ≈ 9.699. Another example for such a relationship is the assignment of EX01_03 to accountability with a MI of ≈ 8.254 and a sepc.all of ≈ -0.256.

The fact which has already been noticed as a result of the residual variance-covariance analysis that especially the items of the three factors DI01, UI01, and EX01 need to be reflected on is further verified through these results. Especially the dimensions of UI01 and DI01 are impacted. In total the following item assignment is suggested according to the modification indices:
<ul>
<li>Accountability: AC01_01, AC01_02, AC01_03</li>
<li>Disclosure: DI01_01, DI01_02, DI01_03, DI01_04, UI01_01, UI01_04, EX01_04</li>
<li>Usefulness: UI01_02, UI01_03 </li>
<li>Explainability: EX01_01, EX01_02, EX01_03</li>
</ul>

Finally, the standardized parameter estimates (factor loadings) of the ordinal CFA are examined to finally decide on how to respecify the model. The factor loadings are evaluated against the rule of thumb of Brown (2015) stating that factor loading can be considered salient if they are > ≈ 0.3 or > ≈ 0.4. For this paper the threshold is set to 0.3.

```{r}
est <- parameterEstimates.mi(ordinal_CFA1, zstat = TRUE)
est
```

Each factor is evaluated in its sequence of appearance, if specific items distinguish from the others of the same factor in any of the following criteria to-be validated, the item is evaluated in a deep dive. The following criteria aligns with the sequence suggested by Brown (2015) on the pages 107-116 which is verified by Kline (2016):
<ol>
<li>Check the magnitude/size of the est: is the factor salient (> 0.3), no out of range values, no Heywood cases, i.e., no factor loadings >1, no negative values (Brown, 2015). If such a case occurs for one of the 95 % confidence intervals, it is noted but not considered a knock-out criteria due to the uncertainty implied by the imputed data foundation (Van Buuren and Groothuis-Oudshoorn, 2011), and the only moderate sample size for WLSMV (Brown, 2015).</li>
<li>Check the direction: all est must be positive for their factor.</li>
<li>Check the se and t-test, i.e., how much sampling error is within the estimates and thus, the stability of the est.</li>
</ol>

As we are conducting the CFA on the basis of multiple imputed data the df-value has to be considered, too. Whereas a low amount of df indicates less reliable values and a high amount of df rather reliable values (Von Hippel, 2018). However, as estimates of df are rather unstable (Von Hippel, 2018), the informativeness of this column is limited.

For the factor of accountability all items seem to be salient according to the est values and ci.lower and ci.upper. Moreover, they inherit all positive est values, i.e., point to the same, positive direction for their accountability factor. Compared to the other factors, they all have a moderate se with the first two items having almost the same se value (≈ 0.074, ≈ 0.073) and the third having a higher se of ≈ 0.091. As a consequence the t-test as a z-score has moderate results, too - assuming a moderate stability. But, as the dfs are relatively low, the estimates are assumed to be unstable due to imputation. However, the analysis of estimates suggests to keep the accountability factor as-is.

For the factor of disclosure only three (DI01_02, DI01_03, DI01_04) out of four items seem to be salient according to the est (DI01_02 (est ≈ 0.988), DI01_03 (est ≈ 0.955), DI01_04 (est ≈ 0.763)) and ci.lower (DI01_02 (est ≈ 0.958), DI01_03 (est ≈ 0.918), DI01_04 (est ≈ 0.660)) and ci.upper for DI01_03 (est ≈ 0.992) and DI01_04 (est ≈ 0.865). It must be noted that for DI01_02 a potential Heywood case for ci.upper ≈ 1.019 is registered - possibly due to the sample size and the imputed data. However, DI01_01 is excluded due to two reasons: undercutting the threshold with an est of ≈ 0.187 and a negative ci.lower of -0.031 (potential Heywood case) which can be in 95 % of the estimates potentially be estimated (Brown, 2015). To continue, the remaining to items, DI02, DI01_03 and DI01_04, all have positive est values and thus, point positively for their factor. DI01_02 has a relatively low se ≈ 0.016 and a high t-value ≈ 62.861 indicating stability which is further underscored through a high df = 1403.331 - declining the importance of the potential Heywood case for ci.upper. For DI01_03 the se is relatively low (≈0.019) and the t-test the overall second highest assuming a high stability. As the df is also moderately high, the stability of this item is verified. For DI01_04 the se is higher (≈0.052) than for DI01_03 and the t-test is lower indicating more unstable results. In contrast, from a imputation point of view it has a considerably higher df of ≈1580.306 indicating an overall stability of the item. Thus, the analysis suggests to keep these three items DI01_02, DI01_03 and DI01_04 for this factor.

Similarly to disclosure, for the factor of usefulness also only two (UI01_01, UI01_02) out of four items are salient according to the est (UI01_01 (est ≈ 0.562), UI01_02 (est ≈ 0.748)) and ci.lower (UI01_01 (est ≈ 0.405), UI01_02 (est ≈ 0.598)) and ci.upper (UI01_01 (est ≈ 0.719), UI01_02 (est ≈ 0.898)). UI01_03 is dropped due to undercutting the threshold of 0.3 potentially twice: in the est with a value of ≈ 0.205 and potentially with the ci.lower ≈ 0.002. For the actual estimation (est), but also for the potential estimations (ci.lower, ci.upper) of UI01_04 Heywood cases of negative factor loadings are prevalent leading to the exclusion of UI01_04. UI01_01 and UI01_02 are both positive est and thus, share the same direction for the factor. Moreover, they share a similar se (≈ 0.078, ≈ 0.076) and t value (≈ 7.244, ≈ 9.891) indicating a rather low stability. They vary in terms of their df with UI01_01 inheriting a significant lower df (≈ 38.491) supporting the low stability argument. Although UI01_02 challenges the low stability argument with a higher df of ≈ 105.897. As a consequence, both remaining items (UI01_01, UI01_02) are kept for the at hand factor.

For explainability the suggestion is made to keep three (EX01_02, EX01_03, EX01_04) out of four items according to the est (EX01_02 (est ≈ 0.505), EX01_03 (est ≈ 0.742), EX01_04 (est ≈ 0.742)) and ci.lower (EX01_02 (est ≈ 0.350), EX01_03 (est ≈ 0.612), EX01_04 (est ≈ 0.845)) and ci.upper (EX01_02 (est ≈ 0.660), EX01_03 (est ≈ 0.871)). It must be noted that for EX01_04 a potential Heywood case for ci.upper ≈ 1.035 is registered - possibly due to the sample size and the imputed data. EX01_01 is omitted due to undercutting the threshold with the est ≈ 0.184 and a potential Heywood case of a negative value for ci.lower. As the remaining items are positive, they share the same direction for their factor. The se of EX01_02 and EX01_03 (≈ 0.079, ≈ 0.066) and t values (≈ 6.398, ≈ 11.238) point to a rather low stability, whereas the df of both are considerably high (≈ 918.774, ≈ 4736.556) indicating stability. In contrast, the se of EX01_04 is relatively low (≈ 0.048) and the t-test is the highest for this factor (≈ 19.428) which indicates the highest stability here. This is underpinned by the highest df ≈ 4736.556 through which the potential Heywood case for ci-upper further declines in importance. Finally, it is suggested to keep three items, EX01_02, EX01_03, and EX01_04 for the explainability factor.

In total the prior observations of the point estimates result in the following picture of the four factor model:
<ul>
<li>Accountability: AC01_01, AC01_02, AC01_03</li>
<li>Disclosure: DI01_02, DI01_03, DI01_04</li>
<li>Usefulness: UI01_01, UI01_02 </li>
<li>Explainability: EX01_02, EX01_03, EX01_04</li>
</ul>


<h2>Summary of Findings for Model Respecification</h2>
In the beginning of the analysis, the absolute fit indices, the parsimony correction index, and the comparative fit indices all indicate a bad model fit rejecting the null hypothesis. 

A deep dive into the model's standardized residual variance-covariance matrix revealed a significant discrepancy of the model for the factors of disclosure, usefulness, and explainability. 

As a consequence, the standardized modification indices are inspected in order to evaluate the impact on the chi-square and thus, on the overall model fit, if adjustments are made. The results suggest to keep the accountability factor as is which is in line with the prior residual analysis. Furthermore, reassigning factors of usefulness and explainability to the factor of disclosure is proposed due to significant cross-loadings. The similarity of these to-be reassigned items is statistically undermined by significant modification indices observed for covariances between items. Overall, the following statistical suggestion for a respecification is made:

<ul>
<li>Accountability: AC01_01, AC01_02, AC01_03</li>
<li>Disclosure: DI01_01, DI01_02, DI01_03, DI01_04, UI01_01, UI01_04, EX01_04</li>
<li>Usefulness: UI01_02, UI01_03 </li>
<li>Explainability: EX01_01, EX01_02, EX01_03</li>
</ul>

To complete the picture, the parameter estimates are evaluated. The estimates suggest to keep the accountability factor as is in line with previous observations. However, the other three factors are proposed to be reduced by two items each leading to the following statistical suggestion for a respecification:

<ul>
<li>Accountability: AC01_01, AC01_02, AC01_03</li>
<li>Disclosure: DI01_02, DI01_03, DI01_04</li>
<li>Usefulness: UI01_01, UI01_02 </li>
<li>Explainability: EX01_02, EX01_03, EX01_04</li>
</ul>

In line with these results the following model respecification is suggested from a statistical point of view. It has to be noted that also the results of the polychoric correlation matrix are considered which state potential connections of items with a high correlation, i.e., multicollinearity (Brown, 2015), and underpin the at hand findings:

<ul>
<li>Accountability: AC01_01, AC01_02, AC01_03</li>
<li>Disclosure: DI01_02, DI01_03, DI01_04, EX01_04</li>
<li>Usefulness: UI01_01, UI01_02 </li>
<li>Explainability: EX01_02, EX01_03, UI01_04</li>
</ul>

Accountability is kept as is which is in line with the observations. Whereas the suggestion of the parameter estimate observation for disclosure is transferred into the respecification expanded by the reassignment of EX01_04. Instead of collapsing the factors with a high multicollinearity (Brown, 2015), the literature suggests to keep all three DI01_02, DI01_03, and EX01_04 distinct as they all point to different aspects of source availability. Thus, the rationale overweighs the statistical observations (Brown, 2015). The UI01_01 assignment to disclosure is omitted fully due to the drop of DI01_01 which is possibly the reason for the suggested reassignment by modification due a high correlation. This correlation is with ≈ 0.55 higher than to the other items of disclosure (≈ 0.13, ≈ 0.15, ≈ 0.16). Moreover, UI01_04 is assigned to explainability instead because of the only minor difference in MI improvement compared to an disclosure assignment and the slightly higher correlation of ≈ 0.62 with EX01_03. In parallel the suggestion of the parameter estimates is applied suggesting to keep only two out of four items.

However, this respecification has to be critically reviewed in line with literature findings as we aim to confirm theory with statistics and not in reverse (Brown, 2015). As accountability is kept as-is, literature findings are confirmed. As the items of disclosure all revere to revealing references the reassignment is in line with literature, too. As usefulness is solely reduced, this is in line with the literature, too. The assignment of UI01_04 is further verified due to the effect of the explainability items which due to revealing steps and interactions request the user to interact. This evaluation can be inspected in more detail within the thesis. 

Nevertheless, it has to be noted that the factors where established for the sake of measuring the impacts on transparency on the basis of literature observations. As transparency is not defined commonly, these factors are also simply constructs of the at hand thesis. Consequently, reassigning items to other factors does not affect answering the research question of an increased transparency at all.

Before continuing to the adjusted model, the potential lack of discriminant validity has to be noted as the factors' covariances in some matter are > 0.85 (Brown, 2015), i.e., accountability x usefulness ≈ 0.875 and disclosure x explainability ≈ 0.986. Therefore, after refining the model the discriminant validity is required to be checked by a comparison of an alternative two factor model with the respecified four factor model (Brown, 2015).

<h1>Respecified Four Factor Model</h1>

The suggested model adjustments are realized below:

```{r}
transparency <- 
" accountability =~ AC01_01 + AC01_02 + AC01_03
disclosure =~ DI01_02 + DI01_03 + DI01_04 + EX01_04
usefulness =~ UI01_01 + UI01_02
explainability =~ EX01_02 + EX01_03 + UI01_04 "

ordinal_CFA2 <- cfa.mi(transparency, data=mids_rel_cols, estimator = "WLSMV", mimic = "MPlus", std.lv = TRUE, ordered = TRUE)
```

<h2>Analysis of parameters and goodness-of-fit indices</h2>

In contrast to the prior CFA, only one instead of ten warnings regarding non-positive definiteness appear. This is a sign that the model refinements work for 9 out of 10 imputed datasets. Subsequently, we are analyzing the improvements. 

```{r}
summary(ordinal_CFA2, standardized = TRUE, fit.measures = TRUE)
```

For the absolute fit indices, the changes of the scaled chi-square value have to be noted firstly as it has decreased from ≈ 217.856 to ≈ 90.564 significantly which already is a sign for an improved fit. The latter is underscored through the comparison of the scaled model chi-square value (≈ 90.564) and the scaled baseline model chi-square value which is considerably higher with ≈ 204.103. Further, the model has a lowered df = 48, whereas the baseline has df = 66. Even though the fit characteristics have improved at this point, the null hypothesis is rejected with a scaled p-value ≈ 0.000 despite the standard p-value ≈ 0.059 which would fail to reject the hypothesis.

The scaled SRMR of ≈ 0.090 is closer to the benchmark of 0.08 than for the initial model with ≈ 0.175. As ordinal data on the basis of imputed data is inspected, we accept this value as approximately close to the benchmark moderate fit. To further check on this, the residual correlations are inspected later on.

Similarly the robust RMSEA decreased from ≈ 0.192 to ≈ 0.136 with a 90 % lower confidence interval of ≈ 0.088 and an upper one of ≈ 0.181. Due to the ordinal and imputed attribute of the data as well as its small sample size for the WLSMV estimator (Brown, 2015), we suggest to accept this value as approximately close to the benchmark (especially for the lower confidence interval) as moderate fit.

For the comparative fit indices have improved, too. The robust CFI ≈ 0.907 is adequate according to the the benchmark of at least 0.9 (Brown, 2015). This benchmark also applies for the (robust) TLI which undercuts its slightly with ≈ 0.872. Despite this and in line with the previous observations and reasons, the values are accepted as moderate fit but cautioned.

As these global indices have been evaluated with the result of an improved, approximately moderate model fit, the model is evaluated against further potential improvements. The analysis is kept short in comparison to the initial analysis in order to come to a fast conclusion on whether the model is kept or refined. The standardized residual variance-covariance matrix is examined first.

```{r}
# cor.bentler residuals, i.e., standardized ones - but $cov.z is needed to show the standardized matrix
res_va_cov_refit <- lavResiduals.mi(ordinal_CFA2, zstat = TRUE)
z_res_va_cov_refit <- lavResiduals.mi(ordinal_CFA2, zstat = TRUE)$cov.z

# raw covariance residuals 
#lavResiduals.mi(ordinal_CFA2, type = "raw", zstat = FALSE)$cov

z_res_va_cov_refit
```

As only minor over- and underestimations are observed which are close to 0 and thus, do not exceed the value of |2.0|, the residual variance-covariance matrix does not suggest any improvements.

```{r}
mod_ind_refit <- modificationIndices.mi(ordinal_CFA2, pool.method = c("D2"), standardized = TRUE, cov.std = TRUE, sort. = TRUE, minimum.value = "4")
mod_ind_refit
view(mod_ind_refit)
```

The modification indices with a peak of ≈ 9.320 for the first row with only minor sepc.all values around 0, also suggest only minor room for improving the model's chi-square value.

```{r}
est_refit <- parameterEstimates.mi(ordinal_CFA2, zstat = TRUE)
est_refit
```
In line with the sequence of analysis for the initial CFA, the est values are analyzed first. The analysis shows that all items are salient, i.e., above the threshold of 0.4 (Brown, 2015). Besides, the point into the same positive direction for each factor. 

It has to be noted that AC01_03 undercuts the threshold for ci.lower slightly and that three of the items (DI01_02, UI01_02, EX01_03) depict potential Heywood cases for their ci.upper values which exceed 1.0. As stated previously, this is not considered a knock-out criteria due to the uncertainty of imputed data (Van Buuren and Groothuis-Oudshoorn, 2011), and the moderate sample size for the WLSMV estimator (Brown, 2015).

The se values range from ≈ 0.015 to ≈ 0.085 indicating a rather small measurement error. Moreover, the t-values ranging from ≈ 6.138 to ≈ 65.441 indicate an adequate stability across the model which is further amplified by dfs ranging from ≈ 50.641 to ≈ 3545.297.

Finally, the parameter estimate analysis verifies what has become apparent already due to prior analysis - that no further adjustments are suggested.

To conclude, the adjusted model proposed an moderate model fit against the background of uncertainties due to the imputed data foundation and the rather small sample size for the WLSMV estimator. Before selecting the adjusted model which is in line with the literature findings to be the finally used model, a comparison to an alternative model needs to be conducted. As accountability and usefulness have a high covariance with ≈ 0.813, a three factor alternative model is analyzed in comparison with the adjusted model to verify its validity for the at hand studies.

<h2>Comparison of the Adjusted 4-Factor Model with an Alternative 3-Factor Model</h2>

First, the alternative model must be initialized on the basis of previous observations indicating a three factor model might be a good fit as the factor covariance of accountability x usefulness exceeds 0.80 with ≈ 0.813 (Brown, 2015). Thus, a three factor model which collapses the factors accountability x usefulness into one factor is suggested as an alternative.

```{r}
transparency <- 
" accountabilityXusefulness =~ AC01_01 + AC01_02 + AC01_03 + UI01_01 + UI01_02
disclosure =~ DI01_02 + DI01_03 + DI01_04 + EX01_04
explainability =~ EX01_02 + EX01_03 + UI01_04 "

ordinal_CFA2_three_f <- cfa.mi(transparency, data=mids_rel_cols, estimator = "WLSMV", mimic = "MPlus", std.lv = TRUE, ordered = TRUE)
```

Initially a simple comparison is conducted on whether the chi-square value has improved. If no significant (p-value < 0.05) improvement can be inspected, the other fit indices are not inspected either as those all only show a different viewpoint on the chi-square value (Brown, 2015; Kline, 2016).

```{r}
model_compare <- lavTestLRT.mi(ordinal_CFA2, ordinal_CFA2_three_f, pool.method = c("D2"), asymptotic = TRUE)
model_compare
```

The alternative model of three factors does not show an significant improvement as Pr(>Chisq) which stands for the p-value ≈ 0.10189 and thus is larger than p = 0.05. The chi-square values only differ by ≈ 6.2086 for 3 df. Moreover, a slight increase of variance and FMI for the alternate model leads to a slightly increased uncertainty, e.g., in estimates.

As a consequence the adjusted four factor model is used as a foundation for the following MANOVA analysis despite the at hand limitations of an only moderate model due to ordinal data and its imputation as well as the total rather small sample size.
