---
title: "Multiple Imputation"
output: html_notebook
---

This is the evaluation of the experiment's results. The focus of this notebook is multiple imputation in preparation of ordinal CFA.

This notebook expands the master thesis by data analysis and respective code as well as short explanations. This markdown does not replace the thesis, it serves solely as an appendix.

<h1>Library Downloads</h1>
```{r}
library(tidyverse)
library(broom)
library(ggplot2)
library(dplyr)
library(finalfit)
library(mice)
library(MASS)
```

<h1>Sample Description</h1>
First, the final sample is loaded:

```{r}
getwd()
raw_data <- readxl::read_excel("data/raw_data.xlsx", sheet = 2)
view(raw_data)
glimpse(raw_data)
```

As discussed within the thesis, the -1 values need to be replaced by plausible values for multiple imputation.  However, they need to be turned into NA values first.

```{r}
relevant_columns <- c("AC01_01", "AC01_02", "AC01_03", "DI01_01", "DI01_02", "DI01_03", "DI01_04", "UI01_01", "UI01_02", "UI01_03", "UI01_04", "EX01_01", "EX01_02", "EX01_03", "EX01_04")

data_with_NA <- raw_data %>%
  mutate(
    across(
      all_of(relevant_columns), ~ na_if(., -1)
    )
  )

glimpse(data_with_NA)
```

In order to find out how much information is missing, the NA values must be counted for all columns from AC01_01 - EX01_04. In addition, the total share of NA values within the answers is calculated.

```{r}
# First drop the col with also NA but referring to socio demographics and thus, textual information
data_with_NA$SD04_14 <- NULL

# Count the NA values
sum_of_NA <- sum(is.na.data.frame(data_with_NA))

# Count the amount of answers for the relevant_columns
relevant_records <- data_with_NA[, relevant_columns]
print(relevant_records)
total_relevant_cells <- nrow(relevant_records) * ncol(relevant_records)

print(paste("Amount of all cells with NA values: ", sum_of_NA))
print(paste("Total answers (with NA values): ", total_relevant_cells))
print(paste("Relative share of NA values in answers: ",  round((sum_of_NA * 100/ total_relevant_cells), 2), "%"))
```

```{r}
df_na <- data.frame(
  item = character(), 
  absolute_amount_of_NA = integer(), 
  relative_amount_of_NA = numeric()
  )

for (col_name in relevant_columns) {
  absolute_NA <- sum(is.na(draft_data[[col_name]]))
  relative_NA <- round(absolute_NA * 100 / nrow(draft_data), 2)
  
  df_na <- rbind(df_na, data.frame(
    item = col_name,
    absolute_amount_of_NA = absolute_NA,
    relative_amount_of_NA = relative_NA
  ))
}

print(df_na)
```

However, this fraction of values that are missing is only an initial starting point. First, we need to transform the items of each dimension into the data type factor. As factors allow us to assign the ordinal character to our data (GeeksforGeeks, 2025), which is required for multiple imputation to apply the correct imputation calculation, i.e., logistic regression (Rubin, 1987).

```{r}
data_with_fct <- data_with_NA %>%
  mutate(
    across(all_of(relevant_columns), as.factor)
  )

glimpse(data_with_fct)
```

Subsequently, since we are using an ordered Likert scale 1-5 (1 = worst value, 5 = best value) an ordering must be added.

```{r}
draft_data <- data_with_fct %>%
  mutate(
    across(
      all_of(relevant_columns), ~ordered(., levels = 1:5)
    )
  )

glimpse(draft_data)
```

A prerequisite to conduct multiple imputation is that the data missing is missing at random (MAR). We already know that only data of the items of transparency are missing (again pointed out below), which is underpinned by the missing values map.

```{r}
draft_data %>%
  missing_plot()
```

The data set relevant_records is used in the following in order to look for pattern of missingness. A pattern for each item of transparency except UI01_02 can be seen (has no missings). However, since proving the MAR is not the focus of the at hand thesis nor possible at all (Heymans & Twisk, 2022), we stop here with the assumption that MAR is given.

```{r}
relevant_records <- draft_data[, relevant_columns]

relevant_records %>%
  missing_pattern()
```

Next, we can start the first imputation using the package mice. According to Van Buuren and Groothuis-Oudshoorn (2011), a dry run is recommended "with the maximum number of iterations maxit set to zero" [p. 35]. With this we can check the method used, the predictor matrix, and various other parts as seen below.

```{r}
ini <- mice(draft_data, maxit = 0)

ini_meth <- ini$method
ini_pred <- ini$predictorMatrix
ini_post <- ini$post
ini_visitS <- ini$visitSequence
ini_seed <- ini$seed

print(ini_meth)
print(ini_pred)
print(ini_post)
print(ini_visitS)
print(ini_seed)

```

The default method is set to "polr", i.e., the ordered factors are recognized correctly, thus, this method is used in the actual imputation. 

The seed is set to NA, but a seed has to be set for replicability purposes which is done subsequently.

The default predictor matrix is enhanced by the method quickpred() (Van Buuren and Groothuis-Oudshoorn, 2011), which follows the rules for predictor selection introduced by Van Buuren, Boshuizen and Knook (1999). Besides, we are explicitly excluding and including parameters which is not done by quickpred() (Van Buuren and Groothuis-Oudshoorn, 2011), but recommended by Van Buuren, Boshuizen and Knook (1999). 

Excluded parameters: NEW CASE NR, TR01_CP, FINISHED
Why?:
Those parameters are system values that are not selected nor influenced by the user at all except for FINISHED which inherits the same value for all records, i.e., has no significant meaning at all.

Explicitly included parameters: SD01 - SD05, TR01, STARTED, LASTDATA, TIME_SUM
Why?:
With the MAR assumption, we assume that the responses are influenced by different matters, e.g., socio demographical background and assigned treatment. Moreover, the completion time and further time-specific data might enhance the view of a person and their behavior, e.g., digital familiarness.

Quick remark:
"A value of 1 indicates that the column variable is used as a predictor to impute the target (row) variable, and a 0 means that it is not used." (Van Buuren and Groothuis-Oudshoorn, 2011, p. 19).

```{r}
predictorMatrix <- quickpred(draft_data, include = c("SD01", "SD02", "SD04", "SD05", "TR01", "STARTED", "LASTDATA", "TIME_SUM"), exclude = c("NEW CASE NR", "TR01_CP", "FINISHED"))

print(predictorMatrix)
```

Now, that the predictor matrix is set, we compute the first actual imputations. To ensure replicability, a seed is set. (Van Buuren and Groothuis-Oudshoorn, 2011) The m value is set to 10 according to the rule of thumb of Rubin (1987). With m = 10 a pilot imputation is conducted in a first place in line with the approach of Von Hippel (2020). The amount of iterations is set to 10 for analysis purposes, e.g., plot examination.

```{r}
set.seed(1001)

pilot_imp <- mice(draft_data, predictorMatrix = predictorMatrix, maxit = 10, method = ini_meth, m = 10, seed = 1001)
```

To ensure that the MI has converged, we are plotting the mids object. It can be seen that the variability of means and SDs for each item stabilizes, i.e., converges. The only item that is less stabilized at first sight is UI01_01. However, this might lead from the fact that only two values are missing. A similar behavior can be inspected for AC01_01, AC03_03, and DI01_04 as they inherit a similar amount of NA values (3, 4, 4) as UI01_01 (2).

```{r}
relevant_imputed_col <- c("AC01_01", "AC01_02", "AC01_03", "DI01_01", "DI01_02", "DI01_03", "DI01_04", "UI01_01", "UI01_03", "UI01_04", "EX01_01", "EX01_02", "EX01_03", "EX01_04")

plot(pilot_imp, c(relevant_imputed_col))
```

Since imputation has worked out, the imputed values are inspected against their plausibility. According to Van Buuren and Groothuis-Oudshoorn (2011) "a good imputed value is a values that could have been observed had it not been missing." [p. 42]. Thus, the densities of all values (observed and imputed ones) of the variables that have been imputed are depicted below.

```{r}
vars <- relevant_imputed_col

stripplot(pilot_imp, as.formula(paste("~", paste(vars, collapse = " + "))), layout = c(1, 1), xlim = c(0, 6))
```
It can be seen that only plausible values are imputed If the observed values (blue) are scattered across 1 to 5, the imputed ones (red) are, too. Especially in UI01_01 it can be observed that the imputation aligns with the observation, i.e., only values between 2 to 5 were observed and only values between 2 to 5 were imputed.

Now that the first pilot is finished, it must be analyzed if the current m achieves an acceptable fraction of missing information (FMI). If this is the case, the imputed object will be used for ordinal CFA and subsequently, for (M)ANOVA. 

Since, we only aim to calculate it exemplary in order to adjust the m of imputations, the FMI is calculated for the sum of all imputed items, i.e., the total score (Heymans and Eekhout, 2019). This is also in line with the observations of Carifio and Perla (2007) stating that Likert scales behave approximately continuous on the macro level, i.e., if the sum of items is conducted. As all imputed items share the same predictors, predictors with the same data type are used in the formula consisting of treatment, socio demographic, and time-related information.

On the basis of the resulting FMI value, the m is kept or set for the actual multiple imputation based on the quadratic rule observations of Bodner (2008) which are also outlined graphically and in numbers by Von Hippel (2018). Since is founded on continuous data and in the at hand paper we conduct approximately continuous data, we assume that the quadratic rule observations also apply in the at hand case for the total mean of imputed items. 

```{r}
pilot_extracted <- complete(pilot_imp, include = TRUE, action = "long")

pilot_extracted$UI01_02 <- NULL

pilot_extracted_mutated <- pilot_extracted %>%
  mutate(
    across(
      all_of(relevant_imputed_col), ~as.integer(.x)
    )
  )

pilot_extracted_mutated <- pilot_extracted %>%
  mutate(
    total_score_imputed_items =
    rowMeans(
      pilot_extracted_mutated[, (relevant_imputed_col)], na.rm = TRUE
    )
  )
glimpse(pilot_extracted_mutated)

```

The adjusted pilot containing the total score is transformed back into a mids object for analyzing the FMI tendency across all imputed items. As we are not inspecting the items on category level but on a macro measurement scale level, we can use continuous parametric methods, such as "lm()" (Carifio and Perla, 2007). Using the general linear model (GLM) we can estimate the FMI on the basis of socio demographics and time-related information.

```{r}
pilot_adjusted <- as.mids(pilot_extracted_mutated)

pilot_fit <- with(pilot_adjusted, lm(total_score_imputed_items ~ TR01 + SD01 + SD02 + SD04 + SD05 + TIME_SUM))
pilot_est <- pool(pilot_fit)
pilot_est$pooled
```

As the intercept FMI of the pilot imputation is ≈0.016 the m must not be increased as the FMI is low. The "ubar" value which is the within-imputation variance indicates a rather low sampling variability. "b" which is the between imputation variance is 0, i.e., the estimation variability due to missing data is not given. This could be due to the underlying structure of the score which ordinal per item and thus, only approximately continuous. In total these two rather low values indicate that the MI is efficient already and reinforce our assumption that the m value must not be increased further. (Von Hippel, 2018)

```{r}
summary(pilot_est, conf.int = TRUE)
```

To achieve a final clarification if the current MI is sufficient, the SE has to be analyzed. Against the backdrop of the tiny sample size of 131 as well as the low share of NA values and consequently, the low FMI - the SE is with ≈0.33 near 0, too. The SE might only decrease further with an increasing sample size (Kaltenbach, 2021), as the FMI is significantly lower than 0.5 whereas an increase of the m would not decrease the SE nor the FMI significantly (Von Hippel, 2018). The Monte Carlo error, which depicts the deviations between the m imputations, is expected to be 0 according to the formula of White et al. (2010) as the between imputation variance "b" is 0.

Consequently, a Monte Carlo simulation would not achieve a significant explanatory contribution is not conducted in this evaluation (Schunk, 2008). This leads us to the conclusion that the pilot MI is already sufficient. Thus, we are exporting the mids object in its original format as well as a .csv for reusability for ordinal CFA and (M)ANOVA.

```{r}
# mids object
saveRDS(pilot_imp, "data/mi_mids_object.rds")

# csv depicting the mids object
final_imputed_dataset <- complete(pilot_imp, include = TRUE, action = "long")
write.csv(final_imputed_dataset, "data/mi_dataset.csv", row.names = FALSE)

```











<h1>Appendix of Multiple Imputation</h1>
To test whether the previous stated argumentation is true, another MI is conducted with a larger m and more iterations.

```{r}
set.seed(1002)

quality_check <- mice(draft_data, predictorMatrix = predictorMatrix, maxit = 25, method = ini_meth, m = 25, seed = 1002)
```

Check for convergence:

```{r}
plot(quality_check, c(relevant_imputed_col))
```

Density check:

```{r}
vars <- relevant_imputed_col

stripplot(quality_check, as.formula(paste("~", paste(vars, collapse = " + "))), layout = c(1, 1), xlim = c(0, 6))
```

FMI check:

```{r}
quality_check_extracted <- complete(quality_check, include = TRUE, action = "long")

quality_check_extracted$UI01_02 <- NULL

quality_checkt_extracted_mutated <- quality_check_extracted %>%
  mutate(
    across(
      all_of(relevant_imputed_col), ~as.integer(.x)
    )
  )

quality_checkt_extracted_mutated <- quality_check_extracted %>%
  mutate(
    total_score_imputed_items =
    rowMeans(
      quality_checkt_extracted_mutated[, (relevant_imputed_col)], na.rm = TRUE
    )
  )
glimpse(quality_checkt_extracted_mutated)
```

FMI analysis:

```{r}
quality_check_adjusted <- as.mids(quality_checkt_extracted_mutated)

quality_check_fit <- with(quality_check_adjusted, lm(total_score_imputed_items ~ TR01 + SD01 + SD02 + SD04 + SD05 + TIME_SUM))
quality_check_est <- pool(quality_check_fit)
quality_check_est$pooled
```

```{r}
summary(quality_check_est, conf.int = TRUE)
```

